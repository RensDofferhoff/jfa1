---
title: "Audit sampling: Evaluating a sample"
author: Koen Derks
date: "last modified: 10-11-2022"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Audit sampling: Evaluating a sample}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{jfa}
  %\VignetteKeywords{audit, evaluation, jfa, planning, prior}
  %\VignettePackage{jfa}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 6)
set.seed(1)
library(jfa)
```

<center>

![Figure 1: The location of the evaluation stage in the audit sampling workflow](img/evaluation.png)\

</center>

This vignette outlines the most commonly used sample evaluation methodology for auditing and shows how to evaluate a sample using the `evaluation()` function in the **jfa** package.

## Evaluation of non-stratified samples

A non-stratified sample evaluation applies when the population is not divided into distinct groups of items (i.e, strata).

### Hypothesis testing

In an audit sampling test the auditor generally assigns performance materiality, $\theta_{max}$, to the population which expresses the maximum tolerable misstatement (as a fraction or a monetary amount). The auditor then inspects a sample of the population to make a decision between the following two hypotheses:

$$H_1:\theta<\theta_{max}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, H_0:\theta\geq\theta_{max}$$. 

The `evaluation()` function allows you to make a statement about the credibility of these two hypotheses after inspecting a sample. Note that this requires that you specify the `materiality` argument in the function.

#### Classical hypothesis testing using the *p*-value

Classical hypothesis testing uses the *p* value to make a decision about whether to reject the hypothesis $H_0$ or not. As an example, consider that an auditor wants to verify whether the population contains less than 5 percent misstatement, implying the hypotheses $H_1:\theta<0.05$ and $H_0:\theta\geq0.05$. They have taken a sample of 100 items, of which 1 contained an error. They set the significance level for the *p* value to 0.05, implying that a *p* value < 0.05 will be enough to reject the hypothesis $H_0$.

```{r}
result_classical <- evaluation(materiality = 0.05, x = 1, n = 100)
summary(result_classical)
```

As we can see, the *p* value is lower than 0.05 implying that the hypothesis $H_0$ is rejected.

#### Bayesian hypothesis testing using the Bayes factor

Bayesian hypothesis testing uses the Bayes factor, $BF_{10}$ or $BF_{01}$, to make a statement about the evidence provided by the sample in support for one of the two hypotheses $H_1$ or $H_0$. The subscript The Bayes factor denotes which hypothesis it favors. By default, the `evaluation()` function returns the value for $BF_{10}$.

As an example of how to interpret the Bayes factor, the value of $BF_{10} = 10$ (provided by the `evaluation()` function) can be interpreted as: *the data are 10 times more likely to have occurred under the hypothesis $H_1:\theta<\theta_{max}$ than under the hypothesis $H_0:\theta\geq\theta_{max}$*. $BF_{10} > 1$ indicates evidence for $H_1$, while $BF_{10} < 1$ indicates evidence for $H_0$. 

| $BF_{10}$ | Strength of evidence |
|---------|-------|
| $< 0.01$ | Extreme evidence for $H_0$ |
| $0.01 - 0.033$ | Very strong evidence for $H_0$ |
| $0.033 - 0.10$ | Strong evidence for $H_0$ |
| $0.10 - 0.33$ | Moderate evidence for $H_0$ |
| $0.33 - 1$ | Anecdotal evidence for $H_0$ |
| $1$ | No evidence for $H_1$ or $H_0$ |
| $1 - 3$ | Anecdotal evidence for $H_1$ |
| $3 - 10$ | Moderate evidence for $H_1$ |
| $10 - 30$ | Strong evidence for $H_1$ |
| $30 - 100$ | Very strong evidence for $H_1$ |
| $> 100$ | Extreme evidence for $H_1$ |

Consider the previous example of an auditor who wants to verify whether the population contains less than 5 percent misstatement, implying the hypotheses $H_1:\theta<0.05$ and $H_0:\theta\geq0.05$. They have taken a sample of 100 items, of which 1 contained an error. The prior distribution is assumed to be a default *beta(1,1)* prior. 

The output below shows that $BF_{10}=515$, implying that there is extreme evidence for $H_1$, the hypothesis that the population contains misstatements lower than 5 percent of the population. 

```{r}
prior <- auditPrior(materiality = 0.05, method = "default", likelihood = "binomial")
result_bayesian <- evaluation(materiality = 0.05, x = 1, n = 100, prior = prior)
summary(result_bayesian)
```

Note that, in audit sampling, the Bayes factor is dependent on the prior distribution for $\theta$. As a rule of thumb, when the prior distribution is very uninformative (as with `method = 'default'`) with respect to $\theta$, the Bayes factor tends to overquantify the evidence in favor of $H_1$. You can mitigate this dependency using `method = "impartial"` in the `auditPrior()` function, which constructs a prior distribution that is impartial with respect to the hypotheses $H_1$ and $H_0$.

The output below shows that $BF_{10}=47$, implying that there is strong evidence for $H_1$, the hypothesis that the population contains misstatements lower than 5 percent of the population. Since the two priors both resulted in convincing Bayes factors, the results are robust to the choice of prior distribution.

```{r}
prior <- auditPrior(materiality = 0.05, method = "impartial", likelihood = "binomial")
result_bayesian <- evaluation(materiality = 0.05, x = 1, n = 100, prior = prior)
summary(result_bayesian)
```

### Estimation

When performing estimation the auditor tries to determine the unknown misstatement in the population on the basis of a sample. Generally, estimation implies that there is a minimal amount of assurance to be obtained about the precision / accuracy of your estimate (i.e., the most likely error - the upper bound). This inference about the population misstatement can be performed using the `evaluation()` function by specifying the `min.precision` argument and providing the sample data or summary statistics.

#### Classical estimation

Suppose your sampling objective is to estimate the misstatement with a precision of 2%. You have planned a sample of *n* = 188 items from which *x* = 1 turns out a contain an error. Standard classical evaluation using the Poisson distribution can be performed using the example code below. Calling the `summary()` function on the result from the `evaluation()` function provides the estimates for the most likely error, the 95% upper bound, and the precision.

```{r}
result_classical <- evaluation(min.precision = 0.02, method = "poisson", n = 188, x = 1)
summary(result_classical)
```

As we can see, the most likely error in the population is 1 / 188 = 0.53% and the 95% (one-sided) confidence interval ranges from 0% to 2.52%. Consequently, the precision of the estimate is 2.52% - 0.53% = 1.99%. This means that this sample provides sufficient information to estimate the misstatement in the population with a precision of 2%.

#### Bayesian estimation

In principle Bayesian estimation follows the same procedure as its classical counterpart, with the exception that a prior distribution must be provided to the `evaluation()` function. Therefore, the first step is to set up a prior distribution (see also the vignette [Prior distributions](https://koenderks.github.io/jfa/articles/v3_prior_distributions.html)). For illustration, we will assume a `default` *gamma(1, 1)* prior distribution. The sample outcomes together with the prior distribution can then be provided to the evaluation function. Once again, the `summary()` function provides the estimates for the most likely error, the 95% upper bound, and the precision. Note that, because the prior is already constructed for use with a `poisson` likelihood, the `method` argument does not need to be provided to the `evaluation()` function.

```{r}
prior <- auditPrior(method = "default", likelihood = "poisson")
result_bayesian <- evaluation(min.precision = 0.02, n = 188, x = 1, prior = prior)
summary(result_bayesian)
```

As we can see, the posterior distribution is a *gamma(2, 189)* distribution. This distribution implies a most likely error in the population is 0.53% and a 95% (one-sided) confidence interval that ranges from 0% to 2.51%. Consequently, the precision of the estimate is 2.51% - 0.53% = 1.98%. Also in the Bayesian framework, this sample provides sufficient information to estimate the misstatement in the population with a precision of 2%.

## Evaluation of stratified samples

A stratified sample evaluation applies when the population is divided into distinct groups of items (i.e, strata). For illustrative purposes, we do not show both classical and Bayesian procedures but the syntax remains the same.

```{r}
data(retailer)
print(retailer)
```

Typically, there are 3 approaches to evaluating a stratified sample: complete pooling, no pooling, and partial pooling (Derks et al., 2022). Complete pooling assumes no difference between strata and all data is aggregated so that it can be analyzed as a whole. No pooling assumes no similarities between strata, which means that all data must be analyzed independently on the level of the strata. Partial pooling assumes differences and similarities between strata, which enables information to be shared between strata. 

### Approach 1: Complete pooling

Complete pooling assumes no differences between strata. This has the advantages that data can be aggregated, which decreases the uncertainty of the population estimate. However, the disadvantage of this approach is that it does not allow the auditor to differentiate between strata, as every stratum receives the same estimate (equal to the population estimate).

```{r}
result_cp <- evaluation(
  materiality = 0.05, method = "binomial", prior = TRUE,
  n = retailer$samples, x = retailer$errors, N.units = retailer$items,
  alternative = "two.sided", pooling = "complete"
)
summary(result_cp)
```

For example, the output shows that the population level estimate is 4.4% with an 95% upper bound of 5.2%. The stratum estimates are all the same.

```{r, echo = FALSE}
plot(1:20, result_cp$strata$mle, pch = 19, ylim = c(0, 0.5), las = 1, main = "Stratum estimates", ylab = "Misstatement", xlab = "Stratum", axes = FALSE)
graphics::axis(side = 1, at = 1:20)
graphics::axis(side = 2, at = seq(0, 0.50, length.out = 6), las = 1)
graphics::arrows(x0 = 1:20, x1 = 1:20, y0 = result_cp$strata$lb, y1 = result_cp$strata$ub, angle = 90, length = 0.05)
graphics::arrows(x0 = 1:20, x1 = 1:20, y1 = result_cp$strata$lb, y0 = result_cp$strata$ub, angle = 90, length = 0.05)
```

### Approach 2: No pooling

No pooling assumes no similarities between strata. This allows the auditor to differentiate between strata, which means that a stratum specific conclusion is possible. However, because all strata are assumed to be independent, this increases the uncertainty of the population estimate.

```{r}
set.seed(1) # Important because the posterior distribution is determined via sampling
result_np <- evaluation(
  materiality = 0.05, method = "binomial", prior = TRUE,
  n = retailer$samples, x = retailer$errors, N.units = retailer$items,
  alternative = "two.sided", pooling = "none"
)
summary(result_np)
```

In this case, the output shows that the population level estimate is 5.6% with an 95% upper bound of 7.6%. The stratum estimates differ from each other.

```{r, echo = FALSE}
plot(1:20, result_np$strata$mle, pch = 19, ylim = c(0, 0.5), las = 1, main = "Stratum estimates", ylab = "Misstatement", xlab = "Stratum", axes = FALSE)
graphics::axis(side = 1, at = 1:20)
graphics::axis(side = 2, at = seq(0, 0.50, length.out = 6), las = 1)
graphics::arrows(x0 = 1:20, x1 = 1:20, y0 = result_np$strata$lb, y1 = result_np$strata$ub, angle = 90, length = 0.05)
graphics::arrows(x0 = 1:20, x1 = 1:20, y1 = result_np$strat$lb, y0 = result_np$strata$ub, angle = 90, length = 0.05)
```

### Approach 3: Partial pooling

Finally, partial pooling assumes differences and similarities between strata. This allows the auditor to differentiate between strata, while also retaining the decreased uncertainty obtained by aggregating information. Partial pooling is a powerful technique that can result in substantially more efficient estimates.

```{r}
set.seed(1) # Important because the posterior distribution is determined via sampling
result_pp <- evaluation(
  materiality = 0.05, method = "binomial", prior = TRUE,
  n = retailer$samples, x = retailer$errors, N.units = retailer$items,
  alternative = "two.sided", pooling = "partial"
)
summary(result_pp)
```

In this case, the output shows that the population level estimate is 4.3% with an 95% upper bound of 5.1%. Like in the previous approach, the stratum estimates are different. However, partial pooling puts the stratum estimates closer together and makes them more accurate.

```{r, echo = FALSE}
plot(1:20, result_pp$strata$mle, pch = 19, ylim = c(0, 0.5), las = 1, main = "Stratum estimates", ylab = "Misstatement", xlab = "Stratum", axes = FALSE)
graphics::axis(side = 1, at = 1:20)
graphics::axis(side = 2, at = seq(0, 0.50, length.out = 6), las = 1)
graphics::arrows(x0 = 1:20, x1 = 1:20, y0 = result_pp$strata$lb, y1 = result_pp$strata$ub, angle = 90, length = 0.05)
graphics::arrows(x0 = 1:20, x1 = 1:20, y1 = result_pp$strat$lb, y0 = result_pp$strata$ub, angle = 90, length = 0.05)
```

## References

* Derks, K., de Swart, J., van Batenburg, P., Wagenmakers, E.-J., and Wetzels, R. (2021). Priors in a Bayesian audit: How integration of existing information into the prior distribution can improve audit transparency and efficiency. *International Journal of Auditing*, 25(3), 621-636.

* Derks, K., de Swart, J., Wagenmakers, E.-J., & Wetzels, R. (2021). The Bayesian Approach to Audit Evidence: Quantifying Statistical Evidence using the Bayes Factor. *PsyArXiv*.

* Derks, K., de Swart, J., Wagenmakers, E.-J., & Wetzels, R. (2022). Bayesian Generalized Linear Modeling for Audit Sampling: How to Incorporate Audit Information into the Statistical Model. *PsyArXiv*.

* Stewart, T. R. (2012). *Technical Notes on the AICPA Audit Guide Audit Sampling*. American Institute of Certified Public Accountants, New York.

* Stewart, T. R. (2013). *A Bayesian Audit Assurance Model with Application to the Component Materiality problem in Group Audits.* VU University, Amsterdam.
