---
title: "Audit sampling: Evaluating a sample"
author: Koen Derks
date: "last modified: 10-11-2022"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Audit sampling: Evaluating a sample}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{jfa}
  %\VignetteKeywords{audit, evaluation, jfa, planning, prior}
  %\VignettePackage{jfa}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 6)
set.seed(1)
library(jfa)
```

<center>

![Figure 1: The location of the evaluation stage in the audit sampling workflow](img/evaluation.png)\

</center>

This vignette outlines the most commonly used sample evaluation methodology for auditing and shows how to evaluate a sample using 
the `evaluation()` function in the **jfa** package. Evaluating an audit sample using `evaluation()` requires that the data from the 
sample is accessible in one of two forms:

- Summary statistics of the sample, including (a vector of) the number of items (`n`), (a vector of) the sum of errors / taints 
  (`x`) and optionally (a vector of) the number of units in the population (`N.units`).
- A `data.frame` containing a numeric column with book values, a numeric column with audit (true) values and optionally a factor 
  column indicating stratum membership.

Typically in audit sampling there is a maximum tolerable error that applies to the population, the performance $\theta_{max}$, which 
should be expressed as a fraction using the `materiality` argument. Next to estimation which is performed by default, specifying 
`materiality` enables the comparison of two competing hypotheses:

$$H_1:\theta<\theta_{max}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, H_0:\theta\geq\theta_{max}$$. 

## Evaluation using summary statistics

### Non-stratified samples

A non-stratified sample evaluation applies when the population is not divided into distinct groups of items (i.e, strata).

#### Classical evaluation using the *p*-value

Classical hypothesis testing uses the *p*-value to make a decision about whether to reject the hypothesis $H_0$ or not. As an example, 
consider that an auditor wants to verify whether the population contains less than 5 percent misstatement, implying the hypotheses 
$H_1:\theta<0.05$ and $H_0:\theta\geq0.05$. They have taken a sample of 100 items, of which 1 contained an error. They set the 
significance level for the *p* value to 0.05, implying that a *p* value < 0.05 will be enough to reject the hypothesis $H_0$.

```{r}
evaluation(materiality = 0.05, x = 1, n = 100)
```

The output shows that the most likely error in the population is estimated to be 1 / 100 = 1% and that the 95% (one-sided) confidence 
interval ranges from 0% to 4.74%. The output also shows that the *p*-value is lower than 0.05 implying that the hypothesis $H_0$ can be rejected and that the sample provides sufficient evidence to conclude that the population does not contain material misstatement.

#### Bayesian evaluation using the Bayes factor

Bayesian hypothesis testing uses the Bayes factor, $BF_{10}$ or $BF_{01}$, to make a statement about the evidence provided by the 
sample in support for one of the two hypotheses $H_1$ or $H_0$. The subscript The Bayes factor denotes which hypothesis it favors. 
By default, the `evaluation()` function returns the value for $BF_{10}$.

As an example of how to interpret the Bayes factor, the value of $BF_{10} = 10$ (provided by the `evaluation()` function) can be 
interpreted as: *the data are 10 times more likely to have occurred under the hypothesis $H_1:\theta<\theta_{max}$ than under the 
hypothesis $H_0:\theta\geq\theta_{max}$*. $BF_{10} > 1$ indicates evidence for $H_1$, while $BF_{10} < 1$ indicates evidence for 
$H_0$. 

| $BF_{10}$ | Strength of evidence |
|---------|-------|
| $< 0.01$ | Extreme evidence for $H_0$ |
| $0.01 - 0.033$ | Very strong evidence for $H_0$ |
| $0.033 - 0.10$ | Strong evidence for $H_0$ |
| $0.10 - 0.33$ | Moderate evidence for $H_0$ |
| $0.33 - 1$ | Anecdotal evidence for $H_0$ |
| $1$ | No evidence for $H_1$ or $H_0$ |
| $1 - 3$ | Anecdotal evidence for $H_1$ |
| $3 - 10$ | Moderate evidence for $H_1$ |
| $10 - 30$ | Strong evidence for $H_1$ |
| $30 - 100$ | Very strong evidence for $H_1$ |
| $> 100$ | Extreme evidence for $H_1$ |

Consider the previous example of an auditor who wants to verify whether the population contains less than 5 percent misstatement, 
implying the hypotheses $H_1:\theta<0.05$ and $H_0:\theta\geq0.05$. They have taken a sample of 100 items, of which 1 was found to 
contain a misstatement. The prior distribution is assumed to be **jfa**'s default *beta(1,1)* prior.

```{r}
prior <- auditPrior(materiality = 0.05, method = "default", likelihood = "binomial")
evaluation(materiality = 0.05, x = 1, n = 100, prior = prior)
```

The output shows that the most likely error in the population is estimated to be 1 / 100 = 1% and that the 95% (one-sided) credible 
interval ranges from 0% to 4.61%. The Bayes factor is shown to be $BF_{10}=515$, meaning that the data from the sample are 515 times 
more likely to occur under the hypothesis of no material misstatement than under the hypothesis of material misstatement.

Note that, in audit sampling, the Bayes factor is dependent on the prior distribution for $\theta$. As a rule of thumb, when the 
prior distribution is very uninformative (as with `method = 'default'`) with respect to $\theta$, the Bayes factor tends to 
overquantify the evidence in favor of $H_1$. You can mitigate this dependency using `method = "impartial"` in the `auditPrior()` 
function, which constructs a prior distribution that is impartial with respect to the hypotheses $H_1$ and $H_0$.

```{r}
prior <- auditPrior(materiality = 0.05, method = "impartial", likelihood = "binomial")
evaluation(materiality = 0.05, x = 1, n = 100, prior = prior)
```

The output shows that $BF_{10}=47$, implying that there is strong evidence for $H_1$, the hypothesis that the population 
contains misstatements lower than 5 percent of the population. Since the two priors both resulted in convincing Bayes factors, the 
results can be considered robust to the choice of prior distribution.

### Stratified samples

A stratified sample evaluation applies when the population is divided into distinct groups of items (i.e, strata). For illustrative 
purposes, this vignette only describes the Bayesian approach but going to a classical approach only requires `prior = FALSE`.

```{r}
data(retailer)
print(retailer)
```

Typically, there are three approaches to evaluating a stratified sample: no pooling, complete pooling, and partial pooling (see Derks et 
al., 2022). No pooling assumes no similarities between strata, which means that all data must be analyzed independently on the level 
of the strata. Complete pooling assumes no difference between strata and all data is aggregated so that it can be analyzed as a whole. 
Partial pooling assumes differences and similarities between strata, which enables information to be shared between strata. 

#### Approach 1: No pooling

No pooling assumes no similarities between strata. This allows the auditor to differentiate between strata, which means that a 
stratum specific conclusion is possible. However, because all strata are assumed to be independent, this increases the uncertainty 
of the population estimate.

```{r}
set.seed(1) # Important because the posterior distribution is determined via sampling
result_np <- evaluation(
  materiality = 0.05, method = "binomial", prior = TRUE,
  n = retailer$samples, x = retailer$errors, N.units = retailer$items,
  alternative = "two.sided", pooling = "none"
)
summary(result_np)
```

In this case, the output shows that the population level estimate is 5.85% with a 95% credible interval ranging from 4.28% to 8.22%. The stratum estimates differ from each other.

```{r, echo = FALSE}
plot(1:20, result_np$strata$mle, pch = 19, ylim = c(0, 0.5), las = 1, main = "Stratum estimates", ylab = "Misstatement", xlab = "Stratum", axes = FALSE)
graphics::axis(side = 1, at = 1:20)
graphics::axis(side = 2, at = seq(0, 0.50, length.out = 6), las = 1)
graphics::arrows(x0 = 1:20, x1 = 1:20, y0 = result_np$strata$lb, y1 = result_np$strata$ub, angle = 90, length = 0.05)
graphics::arrows(x0 = 1:20, x1 = 1:20, y1 = result_np$strat$lb, y0 = result_np$strata$ub, angle = 90, length = 0.05)
```

#### Approach 2: Complete pooling

Complete pooling assumes no differences between strata. This has the advantages that data can be aggregated, which decreases the uncertainty of the population estimate. However, the disadvantage of this approach is that it does not allow the auditor to differentiate between strata, as every stratum receives the same estimate (equal to the population estimate).

```{r}
result_cp <- evaluation(
  materiality = 0.05, method = "binomial", prior = TRUE,
  n = retailer$samples, x = retailer$errors, N.units = retailer$items,
  alternative = "two.sided", pooling = "complete"
)
summary(result_cp)
```

For example, the output shows that the population level estimate is 4.47% with a 95% credible interval ranging from 3.74% to 5.33%. However, the stratum estimates are all the same.

```{r, echo = FALSE}
plot(1:20, result_cp$strata$mle, pch = 19, ylim = c(0, 0.5), las = 1, main = "Stratum estimates", ylab = "Misstatement", xlab = "Stratum", axes = FALSE)
graphics::axis(side = 1, at = 1:20)
graphics::axis(side = 2, at = seq(0, 0.50, length.out = 6), las = 1)
graphics::arrows(x0 = 1:20, x1 = 1:20, y0 = result_cp$strata$lb, y1 = result_cp$strata$ub, angle = 90, length = 0.05)
graphics::arrows(x0 = 1:20, x1 = 1:20, y1 = result_cp$strata$lb, y0 = result_cp$strata$ub, angle = 90, length = 0.05)
```

#### Approach 3: Partial pooling

Finally, partial pooling assumes differences and similarities between strata. This allows the auditor to differentiate between strata, while also retaining the decreased uncertainty obtained by aggregating information. Partial pooling is a powerful technique that can result in more efficient population and stratum estimates.

```{r}
set.seed(1) # Important because the posterior distribution is determined via sampling
result_pp <- evaluation(
  materiality = 0.05, method = "binomial", prior = TRUE,
  n = retailer$samples, x = retailer$errors, N.units = retailer$items,
  alternative = "two.sided", pooling = "partial"
)
summary(result_pp)
```

In this case, the output shows that the population level estimate is 4.34% with a 95% credible interval ranging from 3.45% to 5.33%. Like in the previous approach, the stratum estimates are different. However, partial pooling puts the stratum estimates closer together and makes them more accurate.

```{r, echo = FALSE}
plot(1:20, result_pp$strata$mle, pch = 19, ylim = c(0, 0.5), las = 1, main = "Stratum estimates", ylab = "Misstatement", xlab = "Stratum", axes = FALSE)
graphics::axis(side = 1, at = 1:20)
graphics::axis(side = 2, at = seq(0, 0.50, length.out = 6), las = 1)
graphics::arrows(x0 = 1:20, x1 = 1:20, y0 = result_pp$strata$lb, y1 = result_pp$strata$ub, angle = 90, length = 0.05)
graphics::arrows(x0 = 1:20, x1 = 1:20, y1 = result_pp$strat$lb, y0 = result_pp$strata$ub, angle = 90, length = 0.05)
```

## Evaluation using data



## References

* Derks, K., de Swart, J., van Batenburg, P., Wagenmakers, E.-J., and Wetzels, R. (2021). Priors in a Bayesian audit: How integration of existing information into the prior distribution can improve audit transparency and efficiency. *International Journal of Auditing*, 25(3), 621-636.

* Derks, K., de Swart, J., Wagenmakers, E.-J., & Wetzels, R. (2021). The Bayesian Approach to Audit Evidence: Quantifying Statistical Evidence using the Bayes Factor. *PsyArXiv*.

* Derks, K., de Swart, J., Wagenmakers, E.-J., & Wetzels, R. (2022). Bayesian Generalized Linear Modeling for Audit Sampling: How to Incorporate Audit Information into the Statistical Model. *PsyArXiv*.

* Stewart, T. R. (2012). *Technical Notes on the AICPA Audit Guide Audit Sampling*. American Institute of Certified Public Accountants, New York.

* Stewart, T. R. (2013). *A Bayesian Audit Assurance Model with Application to the Component Materiality problem in Group Audits.* VU University, Amsterdam.
